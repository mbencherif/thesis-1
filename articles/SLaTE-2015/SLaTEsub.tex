\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2015}

\usepackage{graphicx}
\usepackage{amssymb,amsmath,bm}
\usepackage{textcomp}
\usepackage{cleveref}

\def\vec#1{\ensuremath{\bm{{#1}}}}
\def\mat#1{\vec{#1}}

\usepackage[dvipsnames]{xcolor}
\newcommand{\TODO}[1]{{\color{red}\textbf{[TODO #1]}}}

\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tipa}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{subcaption}



\sloppy % better line breaks
\ninept


\title{Automatic classification of lexical stress errors for German CAPT}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% If multiple authors, uncomment and edit the lines shown below.       %%
%% Note that each line must be emphasized {\em } by itself.             %%
%% (by Stephen Martucci, author of spconf.sty).                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\makeatletter
%\def\name#1{\gdef\@name{#1\\}}
%\makeatother
%\name{{\em Firstname1 Lastname1, Firstname2 Lastname2, Firstname3 Lastname3,}\\
%      {\em Firstname4 Lastname4, Firstname5 Lastname5, Firstname6 Lastname6,
%      Firstname7 Lastname7}}
%%%%%%%%%%%%%%% End of required multiple authors changes %%%%%%%%%%%%%%%%%

\makeatletter
\def\name#1{\gdef\@name{#1\\}}
\makeatother \name{{\em%
  Anjana Sofia Vakil%, J\"{u}rgen Trouvain
  %Author Name$^1$, Co-author Name$^2$
  }}

\address{%
  %$^1$Author Affiliation \\
  %$^2$Co-author Affiliation \\
  Department of Computational Linguistics \& Phonetics\\
  Saarland University, Saarbr\"{u}cken, Germany\\
  {\small \tt 
  %[%
  anjanav%
  %,trouvain%
  %]%
  @coli.uni-saarland.de}
}

%\twoauthors{Karen Sp\"{a}rck Jones.}{Department of Speech and Hearing \\
%  Brittania University, Ambridge, Voiceland \\
%  {\small \tt Karen@sh.brittania.edu} }
%  {Rose Tyler}{Department of Linguistics \\
%  University of Speechcity, Speechland \\
%  {\small \tt RTyler@ling.speech.edu} }

%
\begin{document}



  \maketitle
  %
  \begin{abstract}
  
    \TODO{Abstract (200 word limit)}
  
  	Lexical stress plays an important role in the prosody of German, and presents a considerable challenge to native speakers of languages such as French who are learning German as a foreign language. Such learners stand to benefit greatly from Computer-Assisted Pronunciation Training (CAPT) systems which can offer individualized corrective feedback on such errors, and reliable automatic detection of these errors is a prerequisite for developing such systems. With this motivation, this paper explores the automatic classification of non-native German lexical stress errors using machine learning. \TODO{Using a manually-annotated corpus of German speech by native French speakers, a series of experiments were conducted to determine how such errors can most accurately be classified. ....}
    
    
    
  \end{abstract}
  %
  \noindent{\bf Index Terms}: computer-assisted pronunciation training, CAPT, word prosody, German \TODO{are these OK?}


  \section{Introduction}
  
  %\cite{Vakil2015}
  
    
  %Lexical stress, the phenomenon by which a given syllable is accorded a higher level of prominence than other syllables in a given word, serves a contrastive function in some languages (e.g. German) but not others (e.g. French). Lexical stress is very important in German, and may have a large impact on the intelligibility of L2 German speech (see Section 2.4.1); however, given that lexical stress is realized extremely differently (or not at all) in French (see Section 2.3.2), the correct prosodic realization of lexical stress in German is notoriously difficult for L1 French speakers (see Section 2.3.3).
  %
%Computer-Assisted Pronunciation Training (CAPT) systems have the potential to automati- cally provide highly individualized analysis of such learner errors, as well as feedback on how to correct them, and thus to help learners achieve more intelligible pronunciation in the target language (Witt, 2012). The thesis project described here aims to advance German CAPT by creating a tool which will diagnose and offer feedback on lexical stress errors in the L2 German speech of L1 French speakers, in the hopes of ultimately helping these learners become more intelligible when speaking German.
  
  
  For adult learners of a second language (L2), the phonological system of the L2 can pose a variety of difficulties. For certain L2s, such as German or English, one important difficulty involves the accurate prosodic realization of lexical stress, i.e. the accentuation of certain syllable(s) in a given word, with the placement of stress within a word varying freely and carrying a contrastive function in such languages \cite{Cutler2005}. Lexical stress is an important part of German word prosody, and has been found to have an impact on the intelligibility of non-native German speech \cite{Hirschfeld1994}. Coping with this phenomenon in German is especially challenging for native (L1) French speakers, because lexical stress is realized very differently (or perhaps not at all) in the French language \cite{Dupoux2008,Michaux2013}.
  
  To overcome this difficulty and improve their L2 word prosody, learners typically need to have their pronunciation errors pointed out and corrected by a language instructor; unfortunately, the lack of attention typically given to pronunciation in the foreign language classroom, 
  along with other factors such as high student-to-teacher ratios,
  %coupled with the historic lack of instructor training in phonetics/phonology, 
  make this level of individualized attention not always feasible in a classroom setting \cite{Neri2002,Derwing2005,Hirschfeld2007}. Fortunately, advances in Computer-Assisted Pronunciation Training (CAPT) over recent decades have made it possible to automatically provide highly individualized analysis of learners' prosodic errors, as well as feedback on how to correct them, and thus to help learners achieve more intelligible pronunciation in the target language. However, while much research has gone into the creation and improvement of CAPT systems for English (see e.g. \cite{Eskenazi2009,Witt2012}), relatively little work has been done on the development of CAPT systems for German, especially on those targeting errors in German prosody.
  
  This paper describes work that advances the state of German CAPT by applying machine learning methods to the task of diagnosing lexical stress errors in non-native German speech, a necessary prerequisite for delivering individualized corrective feedback on such errors in a CAPT system. The paper is organized as follows: \Cref{sec:bkgd} provides background on the phenomenon of lexical stress as it is realized in German and French word prosody, motivates the creation of CAPT systems that address this error specifically, and summarizes some past work related to this topic. \Cref{sec:data} describes the manual annotation of lexical stress errors in a small corpus of L2 German speech, carried out to create labeled training and test data for the classification experiments explained in \cref{sec:method}. \Cref{sec:results} presents and analyzes the results of these experiments. Finally, \cref{sec:conc} offers some concluding remarks and outlines possible directions for future work.



	\section{Background and related work}
	\label{sec:bkgd}
	
%When there is a typological difference between some segmental or prosodic feature(s) of a language learner’s L1 compared to the target L2, there is a particular need for pronunciation training to bridge this gap. In the case of the French-German language pair, the prosodic realization of lexical stress is one feature which marks a striking difference between the languages.

Broadly speaking, lexical stress is the phenomenon of how a given syllable is accentuated within a word \cite{Cutler2005}, i.e. how a syllable is given a more prominent role such that this syllable is perceived as ``standing out'' \cite{Dogil1999}. This perceived prominence of a syllable is a function not merely of the segmental characteristics of the uttered syllable, i.e. the speech sounds it contains, but rather of its (relative) suprasegmental properties, namely:
\begin{itemize}[topsep=.5em,noitemsep]
\item duration, which equates on the perceptual level to length;
\item fundamental frequency (F0), which corresponds to perceived pitch; and 
\item \TODO{intensity (\textbf{energy?} or amplitude)}, which perceptually equates to loudness.
\end{itemize}

%As Cutler (2005) points out, different languages make use of this suprasegmental information in different ways. 
In variable-stress languages, such as German and English, 
the location of lexical stress in a word is not always predictable,
%it is not always possible to predict which syllable in a word will carry the stress, 
and therefore knowing a word requires, in part, knowing its stress pattern. This allows lexical stress to serve a contrastive function in these languages, 
%such that two words may share exactly the same sequence of phones and nevertheless be distinguished exclusively by their stress pattern, as is the case with 
e.g. distinguishing \textit{UMfahren} (to drive around) from \textit{umFAHRen} (to run over with a car) in German. 
%Because stress carries meaning thus, native speakers of such languages are sensitive to stress patterns, and readily able to perceive differences in stress. 
Furthermore, in German, misplaced stress can disrupt understanding even in cases where there is no stress-based minimal pair \cite{Hirschfeld1994}.
%, supporting the theory that speakers of free-stress languages rely to a large extent on stress information in the recognition of spoken words (Cutler, 2005).
%
However, in fixed-stress languages, stress is completely predictable, as it always falls on a certain position in the word (e.g. the final syllable), making the lexical stress pattern less crucial to the knowledge of a word than in variable-stress languages. 
%Lexical stress may not be as crucial to the knowledge of a word in these languages as in the variable-stress languages. 
Furthermore, 
%although lexical stress is realized in these languages, 
in fixed-stress languages there may be a weaker distinction between stressed and unstressed syllables.
While French has often been categorized as a fixed-stress language, given that word-final syllables are given prominence when a French word is pronounced in isolation, some argue that it may be more properly considered a language without lexical stress, in that speakers do not seem to accentuate any syllable within the word, with word-final lengthening effects explained by interactions with the realization of phrasal accent (lengthening of the final syllable in each prosodic group or phrase) \cite{Dupoux2008,Michaux2013}. 
Regardless, French has no contrastive word-level stress \cite[p.~89]{Michaux2013}, 
and in this respect differs considerably from German.
%which constitutes a significant difference between this language and a language with variable, contrastive lexical stress such as German or English.

This difference between the languages leads us to expect French learners of German to have difficulties with both perception and production of lexical stress prosody.
Although little research has been done on the nature of lexical stress errors for this particular L1-L2 pair, Hirschfeld and Trouvain \cite{Hirschfeld2007} report that such errors are commonly observed in German spoken by French natives.
Research on French speakers' perception of Spanish, another contrastive-stress language, has revealed that these speakers seem to be ``deaf'' to lexical stress, i.e. seem to have significant and lasting difficulty perceiving and remembering stress contrasts \cite{Dupoux2008}. 
With respect to production, studies of L2 Dutch have shown that French speakers, especially beginners, make systematic errors with lexical stress, exhibiting a tendency to stress the final syllable of Dutch words even when stress should be placed on the initial or medial syllable \cite{Michaux2012,Michaux2013}. Similar findings have also been reported for French learners of English \cite{Bonneau2011}.
%As a result of this difference in the sound systems of the two languages, native speakers of French may generally be expected to lack the sensitivity to stress patterns possessed by native speakers of German. Indeed, this has been borne out by research by Dupoux et al. (2008), who found that native French speakers seem to be “deaf” to lexical stress, insofar as they have significant and lasting difficulty perceiving lexical stress in Spanish, another language in which stress serves a contrastive function. This difficulty should also exist for French speakers when they are presented with German words in which the stress pattern is crucial to the word’s meaning, as in the minimal pair above.
%In addition to these difficulties with lexical stress perception, French learners of variable- stress languages such as English, German and Dutch have also been shown to have difficulties in producing stress patterns correctly. Research by Michaux et al. (2012; 2013) revealed that, as might be expected given the tendency for word- and/or phrase-final syllable prominence in French just discussed, French learners of Dutch showed a tendency to stress the final syllables of Dutch words, even when not called for by the canonical stress pattern. Indeed, with regard to German specifically, Hirschfeld and Trouvain (2007) report that lexical stress errors are commonly observed among L2 speakers with French as L1.
%In short, based on prior work on French learners of variable-stress languages, it can reason- ably be expected that L1 French learners of German as L2 will face challenges with both the perception and production of lexical stress, and that the (lack of) lexical stress system in their native language will influence their realization of lexical stress patterns in German words.	
%
	The high (anticipated) frequency of lexical stress errors in the speech of this L1-L2 group is thus one motivating factor for the creation of CAPT systems to help learners identify and correct such errors. 
	
	
	
	Another motivation behind this work's focus on lexical stress errors is the high impact such errors may have on the intelligibility of L2 German speech.
	Intelligibility, as opposed to lack of a “foreign accent,” is generally considered to be the most important goal of pronunciation training \cite{Munro1999,Neri2002,Derwing2005,Field2005,Witt2012}. The exact definition of \textit{intelligibility} is a topic of debate,
	% in the literature, as is the question of whether and how it differs from the notion of comprehensibility; here, let us follow
	but here we will follow Munro and Derwing \cite[p.~289]{Munro1999} in understanding it broadly as ``the extent to which a speaker’s message is actually understood by a listener.''
	Generally speaking, prosodic errors have often been found to have a larger impact on the perceived intelligibility of L2 speakers than segmental errors 
	%Hahn2004
	\cite{Derwing2005,Witt2012}, and several studies have found lexical stress errors to have a particularly strong impact on intelligibility in free-stress languages like English and Dutch \cite{Cutler2005,Field2005} 
	%and German \cite{Hirschfeld1994}, 
	%Dutch, and German (Cutler, 2005; Field, 2005; Hirschfeld, 1994). Indeed, studies on perception of German L2 speech have found that among a variety of pronunciation error types, lexical stress errors have one of the most drastic impacts on intelligibility (Hirschfeld, 1994). 
	Though relatively little research has been done on how various pronunciation errors affect intelligibility in L2 German specifically, some studies suggest that 
	%prosodic errors, and 
	lexical stress errors 
	%especially, 
	may hinder intelligibility of L2 German speech more than other types of errors \cite{Hirschfeld1994,Hirschfeld2007}.
	Stress errors may also affect perception of segmental errors in the L2 learner’s speech; for example, segmental errors occurring in stressed syllables may be more noticeable than those in unstressed syllables \cite{Cutler2005,Michaux2012}. 
	%Additionally, some research indicates that prosodic errors such as lexical stress errors may have more of an impact on perceived foreign accent than segmental errors (Hahn, 2004; Witt, 2012); though it must again be stressed that intelligibility is a more important goal than lack of a foreign accent, insofar as perceived accent may contribute to difficulties being understood by native speakers, this relationship between prosody and accentedness also deserves mentioning.
	It would therefore seem that there may be a strong connection between lexical stress errors and intelligibility in L2 German speech, though more research is needed to clarify the nature of this relationship. \TODO{remove that sentence?}
	
	Though the frequency and impact of lexical stress errors in the speech of French learners of German thus constitute strong reasons to develop CAPT tools to treat such errors, in order for such systems to be viable, the feasibility of reliable automatic detection of this type of error must be demonstrated. 
	\TODO{Make rest of this par about how comparison-based diagnosis is usually used, then start new par with following sentence?}
	To our knowledge, no work has been reported on automatic classification-based diagnosis of lexical stress errors in L2 German speech, 
	%yet related work on other target languages seems encouraging. 
	but in recent years machine learning methods have been applied with apparent success to the classification of lexical stress patterns in English words. 
	Kim and Beutnagel \cite{Kim2011} experimented with various classifiers to identify stress patterns in high-quality recordings of 3- and 4-syllable English words, reporting accuracy in the 80-90\% range; in pilot experiments with low-quality recordings, however, the authors report lower accuracy: 70-80\% on L1 speech and 50-60\% on utterances by L2 speakers. 
	Similarly, Shahin et al. \cite{Shahin2012} trained Neural Networks to classify stress patterns in bisyllabic words uttered by L1 English children, 
	%with the intended application of treating childhood dysprosody, 
	and reported classification accuracy over 90\% for some stress patterns; though this work was conducted with a view to treating childhood L1 dysprosody, its relevance to our intended application of L2 CAPT is nonetheless clear.
	%
	Building on these related investigations, this paper seeks to further explore the viability of automatic classification-based detection of lexical stress errors, with a particular focus on those made by French speakers of German.
	%
	To this end, a small corpus of learner speech was manually annotated for lexical stress errors, as described in \cref{sec:data}.
	Using the resulting labeled L2 data, in addition to data from L1 German speakers, a series of supervised machine learners were trained using a variety of representations of the prosodic and other features of each word utterance (see \cref{sec:method:featuresets}, and these classifiers were evaluated with reference to the manually-produced labels of held-out test data (see \cref{sec:method:datasets}). \Cref{sec:results} presents and analyzes the results of these evaluations.
	
	
	\section{Data}
	\label{sec:data}
	
	Error-annotated speech data from German learners is a prerequisite for the supervised training and evaluation of classifiers for  lexical stress realizations in L2 German speech, yet to our knowledge no corpus of learner German with such annotation is publicly available. To fill this need, as well as to shed light on the perception of lexical stress errors in L2 German speech, a small corpus of speech by L1 French learners of German was manually annotated for such errors by native and non-native German speakers with varying levels of phonetics/phonology expertise. This section describes the data selected for annotation (\cref{sec:data:corpus}) and the method by which lexical stress realizations in this data were annotated (\cref{sec:data:annotation}), and presents an analysis of the observed inter-annotator agreement (\cref{sec:data:agreement}) and distribution of errors (\cref{sec:data:errors}) in the annotated dataset.
	
		\subsection{The IFCASL corpus of learner speech}
		\label{sec:data:corpus}		
		
		The learner speech data used in this work has been excerpted from the IFCASL corpus 
		%Trouvain2013,
		\cite{Fauth2014}, a collection of 
	phonetically diverse utterances in French and German spoken by both native speakers and non-native speakers with the other language as L1. This is the first known corpus of L2 speech in both directions of the French-German language pair, 
	and is thus an invaluable resource for research on pronunciation errors \TODO{between these languages}.
	
	The IFCASL corpus contains recordings of approximately 50 L1 speakers of each language reading carefully constructed sentences (and a short text) in both languages, such that both L1 and L2 speech was recorded for each speaker. Each L1 speaker group has an even gender distribution, and contains approximately 10 children (adolescents of 15-16 years of age) and 40 adults. A variety of self-reported L2 proficiency levels are also represented in the corpus: the recorded adults span levels A2 (beginner) through C1 (advanced) of the Common European Framework of Reference \TODO{footnote url}, the children levels A2 (beginner) and B1 (low intermediate).
	%As described by Fauth et al. (2014) and Trouvain et al. (2013), the IFCASL corpus contains recordings of approximately 100 speakers (50 native speakers of each of the two languages) uttering carefully constructed stimuli in both languages, such that speech in both the L1 and L2 was recorded for each speaker. In recording sessions, subjects were asked to read aloud a given sentence or longer text, and for approximately half of the L2 stimuli, subjects were allowed to listen to a recording of the text as uttered by a native speaker before recording their utterance. An even gender distribution was maintained among the speakers recorded, and both children (adolescents under 18 years of age) and adults participated in the recordings, the majority of participants being adults. While children were uniformly of beginner proficiency in their L2, adults of both beginner and advanced levels in the L2 were recorded. 
		%
	%The IFCASL corpus \citep{Trouvain2013,Fauth2014}, introduced in \cref{sec:intro:ifcasl}, contains recordings of native and non-native speech in French and German, and is thus a invaluable resource for research on pronunciation errors in this language pair, with the subset of the corpus containing L2 German speech by L1 French speakers (henceforth IFCASL-FG) being most relevant to the work reported in this thesis. As described in \cref{sec:intro:ifcasl}, speakers of varying ages, genders, and German proficiency levels are represented in the IFCASL-FG corpus; the exact number and characteristics of these speakers are presented in \cref{tab:data:speakers}.	
	
	%The subsets of the corpus most relevant to the work reported here are those containing German speech, both by L1 French speakers (henceforth IFCASL-FG) as well as by native German speakers (IFCASL-GG) \TODO{confusing sentence?}. 
	While L2 French speech is thus also captured in the IFCASL corpus, the annotation effort described here focuses exclusively on the German-language subset of the corpus. Only utterances from the sub-corpus of L2 German speech by L1 French speakers (henceforth IFCASL-FG) were manually annotated; native utterances from the L1 German sub-corpus (IFCASL-GG) were assumed to contain only correct lexical stress realizations.
	%TODO replace?
%	Details about the speakers included in the IFCASL-FG sub-corpus are given in \cref{tab:data:speakers}.
%	
%	\begin{table}
%			\centering
%			\caption[Speakers in the annotated dataset]{\TODO{remove?} Number of speakers in the portion of the IFCASL-FG corpus annotated for lexical stress, in terms of speakers' age, gender, and proficiency level \cite{Fauth2014}.}
%			\begin{tabular}{lrrrrr}
%			\toprule
%			\multirow{2}{*}{Age/gender}	&	\multicolumn{4}{c}{Proficiency level} &\multirow{2}{*}{\textbf{Totals}}\\
%			\cmidrule(lr){2-5}
%		& A2	&	B1	&	B2	&	C1	&		\\
%			\midrule
%Boy (male, 15-16 yrs.)	&	11	&	0	&	0	&	0	&	\textbf{11}	\\
%Girl (female, 15-16)&	1	&	1	&	0	&	0	&	\textbf{2}	\\
%Man	(male, 18-30) &	7	&	4	&	3	&	7	&	\textbf{21}	\\
%Woman	 (female, 18-30)&	5	&	5	&	3	&	9	&	\textbf{22}	\\
%			%\midrule
%\textbf{Totals}	&	\textbf{24}	&	\textbf{10}	&	\textbf{6}	&	\textbf{16}	&	\textbf{56}	\\
%			\bottomrule
%			\end{tabular}
%			\label{tab:data:speakers}
%		\end{table}
	
	
		
	The subset of IFCASL-FG selected for manual error annotation, (henceforth simply “the dataset”) consists of utterances of twelve bisyllabic word types (see \cref{tab:words}), each of which has primary stress on the initial syllable. 
	%These characteristics were chosen deliberately: the selected words are bisyllabic 
	Only bisyllabic words were selected to simplify comparison between stressed and unstressed syllables, and only initial-stress words because this is the stress pattern which native (L1) French speakers are expected to have the most difficulty producing in German, given the phenomenon of final lengthening in French (see \cref{sec:bkgd}).
	%Though previous work on lexical stress errors in L2 speech has often dealt with words uttered in isolation (e.g. Bonneau and Colotte, 2011), this work deals with word utterances extracted from longer utterances of complete sentences; as Neri et al. (2002, p. 6) point out, for relevance to real communication it is important that CAPT systems address connected speech. The word’s position in the carrier phrase/sentence was not taken into account; although it could be hypothesized that phrase position would have an effect on lexical stress realization by native French speakers, given the phenomenon of phrasal accent in French (see Section 2.3.2), Michaux and Caspers (2013) found no effect of phrase position on French speakers’ realization of words in Dutch. Given the similarities in the lexical stress systems of Dutch and German with respect to French, no such phrase-level effect is therefore expected here, though investigation of the effect of phrase or sentence position on stress realization could be an interesting direction for future work.
	

	
\begin{table}
		\centering
		\caption[Word types annotated for lexical stress errors]{%
		%The twelve bisyllabic initial-stress 
		Word types selected from the IFCASL corpus for lexical stress error annotation. 
		%The orthographic form (text) of each word type is presented in the leftmost column, followed by its canonical pronunciation (e.g. as found in a pronunciation dictionary). 
		Canonical pronunciations for each word type are given in IPA notation.
		% SAMPA notation (\url{http://www.phon.ucl.ac.uk/home/sampa}), with phonemes separated by a space and syllables separated by a period (\texttt{.}); canonical lexical stress is not marked, as all of the listed word types have primary stress on the initial syllable. 
		%Parts of speech are abbreviated as n. (noun), v. (verb) and pro. (pronoun). 
		The rightmost column lists the number of tokens (utterances) of each word type in the annotated dataset.
		}
%		\begin{tabular}{llll}
%		Flagge & Ringen & Tschechen & halten \\
%		M\"{o}rder & Tatort & Fr\"{u}hling & fliegen \\
%		Pollen & manche & E-mail & tragen \\
%		\end{tabular}
		
		{\renewcommand{\arraystretch}{1.1}
		\begin{tabularx}{\columnwidth}{lXXXl}
		%\begin{tabular}{llllc}
		\toprule
		
		Word & 
		%Canonical \linebreak 
		Pronun-ciation & 
		%Pron. &
		Part of speech & 
		%P.O.S. &
		English meaning & 
		%Recording condition \TODO{remove?}& 
		Tokens\\%\linebreak annotated\\
		
		\midrule
		E-mail		
			&	\textipa{/"i:.meIl/} %\texttt{i:~.~m eI l} 		
			&	noun
			&	e-mail %&	SR 	
			&	56	\\
			
		Flagge		
			&	\textipa{/"fla.g@/} %\texttt{f l a~.~g @} 		
			&	noun 
			&	 flag %&	SH	
			&	55	\\
			
		fliegen		
			&	\textipa{/"fli:.g\s{n}/} %\texttt{f l i:~.~g =n} 	
			&	verb 
			&	to fly %&	SR		
			& 56	\\
			
		Frühling	
			&	\textipa{/"fry:.lIN/}	%\texttt{f r y:~.~l I N} 		
			& noun	
			&	spring \newline (season) %&	SR		
			&	56	\\
			
		halten		
			&	\textipa{/"hal.t\s{n}/}	%\texttt{h a l~.~t =n}		
			&	verb 
			&	to hold %&	SR 	
			&	56	\\
			
		manche	
			&	\textipa{/"man.\c{c}@/}	%\texttt{m a n~.~C @} 		
			&	pronoun 
			&	some %& 	SR 	
			&	56	\\
			
		Mörder		
			&	\textipa{/"m\oe5.d5/}	%\texttt{m 96~.~d 6}		
			&	noun 
			&	murderer %&	SR 	
			&	56	\\
			
		Pollen		
			&	\textipa{/"pO.l@n/}	%\texttt{p O~.~l @ n} 		
			&	noun 
			&	pollen %&	SR 	
			& 	55	\\
			
		Ringen		
			&	\textipa{/"KIN.@n/}	%\texttt{r I N~.~@ n}		
			&	noun 
			&	rings %&	SH	
			&	55	\\
			
		Tatort		
			&	\textipa{/"ta:t.PO5t/}	%\texttt{t a:~t~.~?~O6 t}	
			&	noun 
			&	crime scene %& 	SR 	
			&	56	\\
			
		tragen		
			&	\textipa{/"tKa:.g\s{n}/}	%\texttt{t r a:~.~g =n} 	
			&	verb 
			&	to wear %&	SH	
			&	55	\\
			
		Tschechen	
			& \textipa{/"tSE.\c{c}\s{n}/}	%\texttt{tS E~.~C =n}	
			& noun	
			&	Czechs	%&	SR		
			& 56	\\
			
		\bottomrule
		\end{tabularx}
		}
		\label{tab:words}
	\end{table}
	
	In addition to the recordings themselves, the IFCASL corpus contains phone- and word-level segmentations of each utterance, produced automatically by forced alignment with the corresponding text prompts \cite{Fauth2014}. Although the corpus also contains manual corrections of these segmentations, the work reported here relies exclusively on the automatically-generated segmentations to more accurately represent the conditions of a real-world CAPT system, which would not have recourse to manual verification of the phone or word boundaries identified by the aligner. 
	As the IFCASL corpus does not include syllable-level segmentations, boundaries between syllables were determined automatically from the phone- and word-level segmentations of each utterance.
	
	Using the automatic segmentations, tokens (utterances) of each selected word type were extracted from the recorded segmentations automatically using Praat\TODO{footnote url}; counts of the available tokens for each word type are listed in \cref{tab:words}. 
	The dataset annotated for lexical stress errors comprises 668 word tokens in total. Five tokens had to be excluded from the dataset, as sentence-level disfluencies (e.g. false starts or repetitions of phrases) prevented accurate automatic extraction of the word utterance; a fully-fledged CAPT system would need to deal with such disfluencies automatically, e.g. with a pre-processing step  which detects disfluencies and prompts the learner to re-record their utterance if needed 
	(see e.g. %Bonneau2012
	\cite{Orosanu2012}).
	%To compile the dataset, utterances (tokens) of each word as produced by over 50 L2 speakers were extracted from the recordings automatically with Praat\TODO{footnote url}, using extraction times (start and end points of word utterances) taken from the word-level segmentation of each sentence utterance automatically obtained by forced alignment (see Section 4.1). Table 3.2 lists the exact number of tokens available for each word type. In total, 668 word tokens were annotated for lexical stress errors. Five tokens had to be excluded from the data, as disfluencies in the sentence utterance (e.g. false starts or repetitions of the target word) prevented the automatic extraction of the word utterance from the sentence as a whole. In a fully-fledged student-facing CAPT system, such disfluencies would need to be dealt with accordingly, e.g. by means of a pre-processing step which analyzes the student’s utterance for possible disfluencies and compensates for any that are detected by, for example, prompting the student to re-record their utterance. However, detecting disfluencies in speech, especially non-native speech, is a challenging problem under active research (see Section 2.2.1), and the development of a disfluency-aware system is outside the scope of this thesis project; therefore, this work presupposes that no disfluencies exist in the student’s utterance, and the handful of disfluent tokens have been excluded from the dataset described here.
		
		\subsection{Annotation method}
		\label{sec:data:annotation}	
		
		The annotation task consisted of assigning one of the following labels to each word token (utterance) in the dataset described in the previous section:
		\begin{itemize}
	\item{[correct]: the speaker clearly stressed the correct (initial) syllable}
	\item{[incorrect]: the speaker clearly stressed the incorrect (final) syllable}
	\item{[none]: the speaker did not clearly stress either syllable, 
	%i.e. did not audibly differentiate stressed and unstressed syllables, 
	or the annotator was unable to determine which syllable was stressed}
	\item{[bad\_nsylls]: the speaker pronounced an incorrect number of syllables (e.g. inserted an extra syllable), making it impossible to judge if stress was realized correctly}
	\item{[bad\_audio]: a problem with the audio file (e.g. noise or inaccurate segmentation) interfered with the annotator's ability to judge the stress realization}
	 \end{itemize}
	 
	 Annotation was performed using a graphical tool scripted in Praat. This tool displayed the given word's text, and allowed the annotator to listen to the given word utterance and the sentence utterance from which it was extracted as many times as they wished. Once they had reached a judgment about the lexical stress realization of the utterance, the annotator had to click one of five buttons, corresponding to the possible labels, to record their judgment.
	 A single annotation session consisted of annotating all 55-56 tokens of each of three word types, and lasted approximately 15 minutes.
	 %Annotation proceeded by means of a graphical tool scripted in Praat (Boersma and Weenink, 2014), the main interface of which is shown in Figure 3.1. At the top, a word’s text is displayed, along with the IFCASL corpus ID number of the speaker whose utterance of that word will be annotated (this number is only relevant for the annotator insofar as changes in its value inform the annotator that the speaker is changing from utterance to utterance). The recording of the word is played once automatically; the annotator may then choose to click one of the green buttons to play the word again, or play the recording of the entire sentence, as many times as they wish. Once the annotator has judged the accuracy of the lexical stress realization in this utterance, they log that judgment by clicking one of the gray buttons. The annotator is then automatically advanced to the next utterance, with the counts in the lower right corner tracking their progress towards the total number of tokens to be annotated.
		
		A total of 15 annotators participated, varying with respect to their L1 and level of phonetics/phonology expertise. 
		The native languages represented included German (12 annotators), English (2), and Hebrew (1); the L1 English and Hebrew speakers all speak German as \TODO{a} L2. 
		In terms of expertise, the annotators were broadly categorized as \textit{experts} (professional phonetics/phonology researchers), \textit{intermediates} (university students enrolled in an experimental phonology course), or \textit{novices} (those with negligible phonetics/phonology training or experience annotating speech data). Among the 15 annotators, there were two experts, 10 intermediates, and three novices. 		
		%
		%A total of 15 annotators participated in the annotation of this dataset, each of whom is listed in Table 3.3 (by an arbitrary identifier, to preserve anonymity). As Table 3.3 shows, the annotators varied with respect to their native language, as well as with respect to their level of expertise in phonetics/phonology/linguistic annotation.
		%
		%Of the 15 annotators, the majority (12) are native German speakers, and three are non-native (L2) speakers: two are native speakers of American English, and one is a native Hebrew speaker. The L2 German speakers all have some knowledge of German as L2, though the exact German proficiency levels of these annotators are unknown.
		%
		%In terms of expertise, the annotators can broadly be categorized into three groups:
%• expert annotators are professional researchers with a thorough understanding of phonetics/phonology and extensive experience in annotating speech data
%• intermediate annotators are university students enrolled in an experimental phonology course, and have some training in phonetics/phonology and/or experience annotating speech data
%• novice annotators have negligible training in phonetics/phonology and little, if any, experience annotating speech data
%As shown in Table 3.3, the majority of annotators (10 out of 15) fall into the intermediate group; two annotators can be considered expert, and there are three novice annotators.


		
		
		Each annotator was assigned three word types to annotate in a single session, with the exception of one who annotated six word types over two sessions. 
		%Table 3.3 lists the word types assigned to each annotator, along with the number of tokens labeled for each type. Some judgments by annotators D and G had to be excluded from the analysis due to technical problems; the token counts for each annotator in Table 3.3 reflect only their usable judgments.
		Assignments ensured that each word token was annotated by at least two native German speakers, and to maximize the amount of overlap between annotators in order to obtain as many pairwise measures of annotator agreement as possible 
		(see \cref{sec:data:agreement}).
		%(see Section 3.4 for a discussion of inter-annotator agreement); Table 3.4 lists the number of annotators for each word type.
		
		%A single annotation session consisted of annotating all tokens of three word types, and lasted approximately 15 minutes. As mentioned in Section 3.2 above, each annotator participated in one session, with the exception of annotator K who participated in two sessions (separated by several days) and annotated a total of six word types.

	
		%The lexical stress error annotations collected in this manner for each token (utterance) in the dataset enable two important contributions of this thesis, described in the following sections. First, the multiple annotations for each token from different annotators permit an analysis of inter-annotator agreement with regard to the identification of lexical stress errors; Section 3.4 presents this analysis, along with statistics on the relative frequencies with which the five labels were selected by annotators from the different L1 and expertise groups described in Section 3.2. Secondly, and perhaps more importantly, the errors identified by these annotators in the data extracted from the IFCASL corpus enable an analysis of the frequency of lexical stress errors in the speech of L1 French learners of German as L2; this is the subject of Section 3.6.
		
		
		\subsection{Inter-annotator agreement}
		\label{sec:data:agreement}		
		
		Any evaluation of an automatic error detection system, including that described in this work, should be performed with an understanding of the difficulty of the error-detection task for human listeners. To obtain a clearer picture of this task, we therefore conducted an analysis of the inter-annotator agreement observed in the annotations collected as described in the previous section. If human annotators often disagree about whether a given L2 utterance contains a lexical stress error, this may indicate that the task is a difficult one, thus encouraging a more lenient evaluation of an automatic error-detection system. However, if humans are generally in strong agreement, this may reflect a lower level of difficulty, and give reason to judge the performance of an automatic system by a higher standard. 
		%To create a useful CAPT system for lexical stress errors in L2 German, i.e. to automatically detect whether a student has made a lexical stress error in a given utterance, it is helpful to have an understanding of the difficulty of the error-detection task, not only for machines but for humans. It is therefore useful to analyze the collected stress accuracy judgments in terms of inter-annotator agreement, in order to gain insight into the nature of the challenge this task presents. If it is uncommon for human annotators to agree about whether a given lexical stress realization is correct or incorrect, this may indicate that identifying lexical stress errors is a challenging task, and one which an automatic system should also be expected to have difficulty with. If, on the other hand, human annotators are generally in strong agreement, this may reflect a lower level of difficulty, and give reason to judge the performance of an automatic system by a higher standard.
		
		
		For 268 of the 668 utterances annotated, i.e. approximately 40\% of the dataset, annotators were unanimous in their label assignments; for the other 400 utterances (60\%), at least one annotator chose a different label than the other(s) who annotated the same utterance.
		To make sense of these differences, agreement in label assignments was calculated for each pair of annotators who overlapped, i.e. labeled any of the same tokens. 
		Pairwise agreement was quantified in terms of percentage agreement (i.e. the number of tokens to which the two annotators assigned the same label, divided by the total number of tokens they both annotated), and Cohen's Kappa ($\kappa$) statistic \cite{Cohen1960}.
		To obtain an overall measure of inter-annotator agreement for the entire annotated dataset, the agreement between each pair of overlapping annotators was calculated, and the minimum, median, mean, and maximum values over all pairwise comparisons were computed; these values are given in \cref{tab:agreement:overall}.
		
		\begin{table}
			\centering
			\caption{Overall pairwise agreement between annotators}
			\begin{tabular}{lrr}
			\toprule
				&	\% Agreement	&	Cohen's $\kappa$	\\
			\midrule
Mean		&	54.92\%	&	0.23	\\
Maximum	&	83.93\%	&	0.61	\\
Median		&	55.36\%	&	0.26	\\
Minimum	&	23.21\%	&	-0.01	\\
			\bottomrule
			\end{tabular}						
			\label{tab:agreement:overall}
		\end{table}
		
		This simple analysis reveals a few interesting observations. First, the mean and median percentage agreement values near 55\% indicate that annotators seem to agree about the accuracy of lexical stress realizations just slightly more than they disagree, and the mean and median $\kappa$ values near 0.25 characterizes the overall agreement as ``fair'' in the Landis and Koch schema \cite{Landis1977}. However, the minimum and maximum $\kappa$ values reveal that agreement between different pairs of annotators ranges from ``poor'' to ``substantial'' \cite{Landis1977}, as also reflected in the correspondingly large gap between the minimum and maximum percentage agreement observed.
	%Turning to the $\kappa$ values, given that $\kappa = 0$ represents agreement purely by chance while $k = 1$ represents perfect, meaningful agreement, the fact that the mean and median $\kappa$ values between annotators are somewhere near 0.25 indicates that the agreement observed between annotators is closer to what would be expected simply by chance than to agreement that would indicate highly reliable annotations, and signifies ``fair'' agreement between annotators according to the thresholds proposed by \textcite{Landis1977}. Examination of the minimum and maximum values reveals that while some pairs of annotators seem to exhibit ``substantial'' agreement, indicating reasonably reliable judgments, other pairs have ``poor'' agreement; in one case, with 23.21\% agreement, the annotators seem to be closer to perfect disagreement than perfect agreement, and the corresponding $\kappa$ being below zero indicates that they agreed even (slightly) less than one would expect if they were merely labeling utterances randomly. 
		On the whole, then, it appears that inter-annotator agreement in this error annotation task is relatively low, though there seems to be considerable variation between individual annotators. This may simply signal that (some of) the particular annotators participating in this study are not very reliable in their judgments of lexical stress accuracy, but it may also indicate that diagnosing errors in L1 French speakers' realizations of lexical stress in German is a difficult task, even for humans.
		%; this notion will be revisited in \cref{sec:diag:classification}. 
	
	

		
		
		
		%To put the agreement values reported in this section in context, it is worthwhile to compare them with those obtained in recent work by \textcite{Michaux2013} on L2 Dutch speech by L1 French speakers. In this study, three expert annotators were asked to indicate which syllable they perceived as stressed in Dutch word utterances by the L1 French speakers. Two of the annotators were L1 Dutch speakers, the other an L1 French speaker who was nevertheless ``highly proficient'' in Dutch (p.~90). The authors report an average $\kappa$ of 0.71 between annotators, representing ``substantial'' agreement in the schema of \textcite{Landis1977}, and a much higher value than those observed in the present work. One plausible explanation for this difference may be the fact that while the three annotators participating in the \textcite{Michaux2013} study were all ``phonetically trained'' (p.~90), i.e. experts (see \cref{sec:lexstress:annotators}), the annotation project described here collected judgments from a larger number of annotators, and only two of the 15 participating annotators had extensive phonetic training. The relationship between annotator expertise and inter-annotator agreement is explored further in \cref{sec:agreement:expert}. 
		
			
		
		
		\subsection{Error distribution}
		\label{sec:data:errors}
		
		Though the comparison of the (potentially different) labels assigned to each word utterance by different annotators is interesting, a single ``gold-standard'' label for each utterance ultimately had to be determined, as a representation of the ground truth with which to train and evaluate the automatic error classifier(s) \TODO{rewrite that sentence}.
		In some cases, assigning a gold-standard label was trivial, e.g. when all or a majority of annotators agreed. However, in other cases a choice had to be made between competing candidate labels. 
		%In these cases the gold-standard label was chosen via a rule-based procedure which 
		\TODO{The decision-making procedure} prioritized experts' judgments, favored certain  judgments ([correct] or [incorrect]) over uncertain ([none]), and gave learners the benefit of the doubt when annotators disagreed as to whether the utterance was [correct] or [incorrect].
	
		
		To shed further light on the error classification task, the overall distribution of lexical stress errors in the annotated dataset 
		was analyzed with reference to the gold-standard labels thus determined.
		As 
		%seen in \cref{tab:errors} and 
		illustrated in \cref{fig:errors}, 
		%Evidently, 
		most of learners' lexical stress realizations were judged to be correct (426 utterances, i.e. 63.8\% of the 668 labeled utterances);
		%most (63.77\%) of learners' lexical stress productions were judged to be correct; 
		in other words, almost two-thirds of the time, learners clearly stressed the correct (initial) syllable in the uttered word.
		However, learners also seemed to make mistakes regularly, with 29.6\% (198) of their word utterances labeled [incorrect] and another 5.2\% (35) labeled [none].
		%However, incorrect productions (productions in which the learner clearly stressed the incorrect syllable) and productions in which the learner did not clearly stress either syllable (corresponding to the [none] label, as described in \cref{sec:lexstress:method}), also occurred regularly: 29.64\% of the productions were judged incorrect and 5.24\% were labeled [none]. 	
		If we consider both [incorrect] and [none] utterances as types of lexical stress errors, then errors were observed in just over one-third of utterances. %(34.88\%) of the  utterances annotated. 
		%
		This considerable proportion of errors seems to confirm the expectation (mentioned in \cref{sec:bkgd}) that French learners of German frequently make lexical stress errors.
		Only 1.2\% (8) of the word utterances were deemed to have an incorrect number of syllables, and a problem with the audio was indicated in only one utterance; this small proportion of impossible-to-evaluate utterances seems to bode well for an automatic error detection system, \TODO{which will have to cope with such errors}.
		
%		\begin{table}
%			\centering
%			\caption{Overall frequency of lexical stress errors in the annotated data \TODO{remove}}
%			\begin{tabular}{lrr}
%			\toprule
%			Label & Tokens & \% of corpus \\
%			\midrule
%			correct	& 426	& 63.77\% \\
%			incorrect &	198	& 29.64\% \\
%			none	 &35 &	5.24\% \\
%			bad\_nsylls	& 8	& 1.20\% \\
%			bad\_audio	& 1	& 0.15\%\\
%			\midrule
%			%\addlinespace
%			Total & 668 & 100\%\\
%			\bottomrule
%			\end{tabular}
%			\label{tab:errors}
%		\end{table}
		
		\begin{figure}
			\centering
			\includegraphics[width=\columnwidth]{overallJudgments-axisTop-noLabels}
			\caption{Overall distribution of lexical stress errors in the annotated data}
			\label{fig:errors}
		\end{figure}
	
	
	
	
	\section{Method}
	\label{sec:method}
	
	As mentioned in \cref{sec:bkgd}, the classification-based approach to identifying lexical stress errors has not been sufficiently explored in CAPT research, especially research on CAPT for German. 
	By way of a preliminary investigation of the feasibility of this type of error diagnosis, a series of classification experiments were conducted in an effort to determine:
	\TODO{
	\begin{itemize}%[topsep=-.5em]
	 \item{how accurately lexical errors can be automatically classified, in comparison to the accuracy of human listeners in identifying such errors,}
	 \item{which of the prosodic features discussed in \cref{sec:diag:prosody} are most useful for classification, and}
	 \item{whether a classification-based approach can lead to reasonably accurate diagnosis for words or speakers not seen in the training data.}
	 \end{itemize}
	}
	% The remainder of this section describes these experiments, and \cref{sec:results} presents their findings. 
	
	 
	 The WEKA machine learning toolkit\TODO{footnote URL} was used to train and evaluate classifiers for these experiments. In this work, only simple Classification And Regression Tree (CART) classifiers \TODO{Breiman1984?} were used.
	 Many other classification algorithms are implemented in WEKA, some of which could  conceivably offer better performance; here, CARTs were chosen for their simple training process and their ease of interpretation by humans. 
	 In future work (see \cref{sec:conc}), it would be interesting to compare different classification algorithms to see if other classifiers are more effective for this type of data, along the lines of the experiments by Kim and Beutnagel \cite{Kim2011}.

		\TODO{paraphrase the below}

		For each relevant configuration (see \cref{sec:method:featuresets,sec:method:datasets}), a CART is trained to classify utterances as belonging to one of the five label categories described in \cref{sec:data:annotation}. 
		In practice, however, these trees classify every utterance as either [correct] or [incorrect], neglecting [none] and the other labels due to their comparatively low frequency in the data. 
		Overall classification accuracy on the annotated sub-corpus was assessed by using held-out portions of the annotated data as test sets, and performing cross-evaluation on multiple train/test splits of the data. The features and divisions of data used in each experiment are described in the sections below.
	
	
	    \subsection{Feature sets}
	    \label{sec:method:featuresets}
	    
	    To represent the lexical stress prosody of an utterance, the automatically-determined word, syllable, and phone segmentations were used to isolate relevant segments of the speech signal, and extract features related to the three acoustic properties mentioned in \cref{sec:bkgd} above: duration, fundamental frequency (F0), and energy. To account for inter-speaker variability, e.g. the fact that some speakers may have a faster speech rate or higher F0 than others, relative rather than absolute features were used. This section explains how these prosodic features, listed in \cref{tab:prosfeatures}, were selected and computed.
	    %The automatically-determined word, syllable, and phone boundaries obtained as described in the previous section enable the CAPT tool to locate and analyze segments of the speech signal relevant to the realization of lexical stress. This section describes the features by which the system analyzes the lexical stress prosody of an utterance, be it the utterance of a learner or of a native speaker. These features relate to the three acoustic properties (and by extension their perceptual correlates) described in Section 2.3, namely duration (timing), fundamental frequency (pitch), and intensity (loudness). The relative utility of these features in automatically diagnosing lexical stress errors is discussed further in Section 4.4.
	    
	    Research on the phonetic realization of lexical stress has often indicated that duration may be the most important, if not the only, acoustic correlate of this phenomenon in German, with the duration of stressed syllables being relatively long in comparison with unstressed syllables \cite{Dogil1999}. 
	    Therefore, features representing duration were computed, simply by noting the relative durations of relevant segments in the phone- and syllable-level segmentations. Following Bonneau and Colotte~\cite{Bonneau2011}, we take into account the durations of entire syllables, as well as of their nuclei (vowels or syllabic consonants such as /\textipa{\s{n}}/), as described in \cref{tab:durfeatures}.
	    %Analysis of duration (timing) is extremely important for detecting stress patterns; indeed, some research indicates that syllable duration may be the most important, if not the only acoustic correlate of lexical stress in German (e.g. \cite{Dogil1999}). Duration analysis therefore figures prominently in the analysis and assessment of learners' lexical stress in this work. 
	    %
	    %Given the word-, syllable- and phone-level segmentations of an utterance (see \cref{sec:diag:segmentation}), the extraction of duration features for that utterance is trivial, as it consists simply of noting the duration of each relevant segment. % in the appropriate segmentation. Following \textcite{Bonneau2011}, duration analysis in this work takes into account the  duration of each syllable in the word to be analyzed, and of the vowels at the nucleus of each syllable. 	To account for inter-speaker variability, e.g. the fact that some speakers may have an overall slower or faster speech rate than others, relative rather than absolute durations are used. The list of duration features computed for each word utterance is given in \cref{tab:durationfeatures}, along with the values computed for each feature from the sample utterances shown in \cref{fig:featuresexample}.
	    %
	    
	   After duration, the next best acoustic correlate of lexical stress appears to be F0 \cite{Dogil1999}, so F0 features were also computed (see \cref{tab:f0features}).
	    The F0 contour of a given utterance is estimated using the pitch detection functionality of the speech-processing program JSnoori\TODO{footnote URL} \cite{DiMartino1999}, which uses a spectral comb method to compute pitch points from each FFT spectrum extracted from the relevant signal segment; here spectra were extracted using Hamming windows 32-milliseconds long, offset by 8 ms. Points are computed in Hz, then converted to semitones before features are computed. Features only take into account non-zero points, i.e. those corresponding to voiced segments.
	    Though work on assessing L2 English stress has often made the assumption that stressed syllables should have higher F0 than unstressed ones (e.g. \cite{Bonneau2011}), in German stressed syllables may also have a lower F0 than other syllable(s) in the word \cite[p.~267]{Cutler2005}. Therefore, as \cref{tab:f0features} shows, our features capture not only the maximum F0 in each syllable (nucleus), but also the minimum and range (maximum$-$minimum).
	    %Much of the work on assessing non-native lexical stress has been conducted with English as the L2, and thus often makes the assumption that a stressed syllable should have a higher F0 than unstressed syllables (Bonneau and Colotte, 2011). In German, the F0 of a stressed syllable also tends to differ from the surrounding contour, but the difference may be positive (the stressed syllable has a higher pitch than surrounding syllables) or negative (lower pitch) (Cutler, 2005, p. 267). Therefore, the computed features capture not only the F0 maximum of each syllable, but also the minimum and range (difference between maximum and minimum).
	    
	    Past research indicates that a signal's energy (intensity) also reflects lexical stress patterns, though to a lesser extent than duration or F0 \cite{Dogil1999,Cutler2005}.
	   % Research on lexical stress prosody has generally indicated that intensity is the least important of the three features, i.e. corresponds least closely to lexical stress patterns (Cutler, 2005). Indeed, existing lexical stress assessment tools may not take intensity into account, as was the case with the prosodic diagnosis functionality of JSnoori at the start of this thesis project. However, intensity can nonetheless have an impact on the perception of lexical stress, especially in combination with pitch or duration, or both (ibid.); Therefore, in addition to duration and fundamental frequency, JSnoori’s learner speech analysis module was modified to take intensity (energy)2 into account.
	   Therefore, energy contours of syllables and their nuclei were also computed, again using JSnoori, which calculates the total amount of energy at frequencies from 0 to 8000 Hz in FFT spectra extracted from the signal (using Hamming windows 20 ms. long offset by 4 ms). Energy values below 60 decibels are not counted toward the total, as these are assumed to correspond to ambient or non-speech noise. Using these contours, we compute features taking into account the mean and maximum energy in the relevant segments, as shown in \cref{tab:enerfeatures}.
	    
	    \begin{table}
		\centering
		%\caption[Features used for duration analysis]{Features used for duration analysis, and their values for the sample utterances of ``Flagge'' in \cref{fig:featuresexample}. 
		\caption{%
		Prosodic features.
		%Features used to represent lexical stress prosody. 
		S0 refers to the word's first syllable, S1 to the second syllable; similarly, V0 and V1 refer to the nucleus (vowel) of the first and second syllable, respectively.
		}
		
		\begin{subtable}{\columnwidth}
		\centering
		\caption{Duration (DR) feature set}
		 \begin{tabularx}{\columnwidth}{lX}%{lXcc}
		\toprule
		Feature name & Description \\
		%\multirow{2}{*}{Feature name} 
		%& \multirow{2}{*}{Description}
		%& \multicolumn{2}{c}{Value} \\
		%&		 &  (a) G		& (b) F \\
		\midrule
		REL-S-DR 	
			& \mbox{Duration of S1}$/$\mbox{duration of S0}
			%& 	0.50		& 	1.00	
			\\
		REL-V-DR 		
			& \mbox{Duration of V1}$/$\mbox{duration of V0}
			%& 	0.56		& 	1.71	
			\\
		\bottomrule	
		\end{tabularx}
		\label{tab:durfeatures}
		\end{subtable}
		
		\vspace{1em}
		
		\begin{subtable}{\columnwidth}
		\caption{Fundamental frequency (F0) feature set}
		{\renewcommand{\arraystretch}{1.25}%
		\begin{tabularx}{\columnwidth}{lX}%{lXrr}
		\toprule
		Feature name & Description \\
		\midrule
		REL-S-F0-MEAN 
			& Mean F0 in S1$/$ mean F0 in S0 \\
		REL-S-F0-MAX 
			& Maximum F0 in S1$/$ max. F0 in S0 \\
		REL-S-F0-MIN 
			& Minimum F0 %($>$0) 
				in S1$/$ min. F0 in S0 \\
		REL-S-F0-RANGE
			& F0 Range (max. F0$-$min. F0) in S1$/$ F0 range in S0 \\
		
		REL-V-F0-MEAN 
			& Mean F0 in V1$/$mean F0 in V0 \\
		REL-V-F0-MAX 
			& Max. F0 in V1$/$max. F0 in V0 \\
		REL-V-F0-MIN 
			& Min. F0 in V1$/$min. F0 in V0 \\
		REL-V-F0-RANGE 
			& F0 Range in V1$/$F0 range in V0 \\
		
%		F0-MAX-INDEX	
%			& $\begin{cases}
%					0 & \text{if max. F0 in S0}>\text{max. F0 in S1}\\
%					1 & \text{otherwise}\\
%				\end{cases}$ \\
%		F0-MIN-INDEX	
%			& $\begin{cases}
%					0 & \text{if min. F0 in S0}<\text{min. F0 in S1}\\
%					1 & \text{otherwise}\\
%				\end{cases}$ \\
%		F0-MAXRANGE-INDEX	
%			& $\begin{cases}
%					0 & \text{if F0 range in S0}>\text{F0 range in S1}\\
%					1 & \text{otherwise}\\
%				\end{cases}$ \\
		
		\bottomrule
		\end{tabularx}
		} % end arraystretch
		\label{tab:f0features}
		\end{subtable}
		
		
		\vspace{1em}
		
		\begin{subtable}{\columnwidth}
		\caption{Energy (EN) feature set}
		\begin{tabularx}{\columnwidth}{Xl}%{lXrr}
		\toprule
		Feature name & Description \\
		\midrule
		REL-S-EN-MEAN 
			& Mean energy in S1$/$S0 mean en.
			%&  0.39 & 0.38 
			\\
			
		REL-S-EN-MAX 
			& Maximum en. in S1$/$S0 max. en.
			%&  0.52 & 0.45
			\\
			
		REL-V-EN-MEAN 
			& Mean en. in V1$/$mean en. in V0
			%&  0.43 	& 0.43
			\\
			
		REL-V-EN-MAX 
			& Max. en. in S1$/$max. en. in S0
			%&  0.53 	& 0.61
			\\
			
%		EN-MAX-INDEX 
%			& $\begin{cases}
%					0 & \text{if S0 max. en.}>\text{S1 max. en.}\\
%					1 & \text{otherwise}\\
%				\end{cases}$
%			& 0	& 0\\
			
		\bottomrule
		\end{tabularx}
		\label{tab:enerfeatures}
		\end{subtable}
		
		
		\label{tab:prosfeatures}
		\end{table}
		
		 Using different combinations of these feature sets, a series of experiments was conducted to determine which features give the best accuracy in the error-classification task. Establishing which features perform best not only enables the creation of the most accurate classification-based error diagnosis system possible, but may also clarify whether and how strongly these acoustic properties correspond with perceived lexical stress errors in L2 German.
		 In addition to the prosodic features (\cref{tab:prosfeatures}), features representing the word type uttered and characteristics of the speaker (see \cref{tab:spkrfeatures}) were also included in the experiments, to ascertain whether including such features could improve performance.
		The results of these feature experiments are presented in \cref{sec:results:features}.
		
		\begin{table}
			\centering
			\caption{Speaker/word features}
			\begin{tabularx}{\columnwidth}{lX}
			%\begin{tabular}{ll}
			\toprule
			Feature name & Description \\
			\midrule
			WD %WORD 
				& Word type uttered (e.g. \textit{Tatort}) \\
			LV % LEVEL 
				& Speaker's L2 German skill level \newline (A2$|$B1$|$B2$|$C1)\\
			AG %GENDER 
				& Speaker's age/gender category \newline (Girl$|$Boy$|$Woman$|$Man)\\
%			LV+AG & LEVEL, GENDER \\
%			WD+LV & WORD, LEVEL \\
%			WD+AG & WORD, GENDER \\
%			WD+SPKR & WORD, LEVEL, GENDER \\
			\bottomrule
			\end{tabularx}
			\label{tab:spkrfeatures}		
		\end{table}
	    
	    
	    \subsection{Datasets for training and testing}
	    \label{sec:method:datasets}

		As mentioned above, CART classifiers were trained and tested using the labeled dataset described in \cref{sec:data}. In addition to this L2 data, L1 utterances of the selected word types from the the IFCASL-GG corpus were also included in training data, each having been automatically labeled as [correct] based on the assumption that native speakers always realize lexical stress correctly.


		To evaluate the performance of the features described in \cref{sec:method:featuresets}, 10-fold cross-validation was performed on the entire set of available training data. The current objective being the classification of L2 speech, including L1 utterances in the test data was not appropriate; instead, to create each of the 10 folds, one-tenth of the L2 utterances were randomly selected to be held out as the test data, and the corresponding training set consisted of the remaining nine-tenths of the L2 utterances combined with the entire set of L1 utterances. Overall classification accuracy was computed by averaging the results over each of these 10 folds (see \cref{sec:results:features}).
		%A 10-fold cross-validation was performed on the entire set of available training data, i.e. the utterances of 12 German word types produced by L1 French speakers which had been annotated for lexical stress errors as described in Chapter 3, along with the utterances of these word types by native German speakers, labeled as correct. As the goal of error diagnosis by classification in the CAPT context is to classify non-native speech, including native utterances in the test data was not appropriate; therefore, for each of the 10 folds of the cross-validation, one tenth of the non-native utterances were randomly selected to be held out as the test data set, and the other nine-tenths were combined with the native utterances to create the training data set.
		%To evaluate feature performance, classifiers were trained using various subsets of the com- plete feature set, which are listed in Table 4.4. For each of these feature combinations, classifiers were trained on each of the 10 training sets created as just described, and tested on the corresponding test set. The averages of each of the aforementioned evaluation metrics (see Section 4.4.1) across all 10 folds are reported.
		
		\TODO{Compared to comparison-based diagnosis}, a classification-based approach to error diagnosis is attractive in that it can theoretically enable the creation of CAPT exercises for new word types without requiring additional recordings of L1 utterances of these words. 
		To assess whether this extension to new words is really feasible, we also evaluated classification performance on words not seen in the training data. This was accomplished by dividing the annotated dataset into 12 subsets, one for word type (see \cref{tab:words}), and performing a 12-fold cross-validation. For each fold, the test dataset consisted of the L2 utterances of a single held-out word type, while the training dataset comprised the L1 and L2 utterances of the other 11 words. 
		%one motivation for a classification-based approach to error diagnosis is that the abstraction from concrete reference utterances afforded by such a method can theoretically enable the diagnosis of errors in new word types without requiring additional recordings of native speakers pronouncing these words; to assess the feasibility of diagnosing utterances of new words, the classifier’s performance on words not seen in the training data must be evaluated and compared with the performance observed on seen words, with the hypothesis that the former will be better than the latter. 	
		%To evaluate classification performance on unseen word types, the non-native utterances annotated for lexical stress (see Section 4.4.1) were divided by their word type, resulting in twelve different data sets to be used for testing. To construct the training set for each test set, utterances of the target word type were removed from the complete set of native German utterances, and the resulting set of 11 word types uttered by L1 German speakers was combined with the utterances of those 11 word types by L2 speakers. Given the improvement in performance observed with the inclusion of speaker-related features (see Section 4.4.2), along with the apparent interplay between such features and the prosodic feature set (such that overall performance improved when all prosodic features were included, not just the best-performing prosodic features of duration and F0), classifiers were trained using a few different combinations of features, to ascertain how these features affect performance when the test data consists of unseen words. It might be the case that including information about the speaker, i.e. their age/gender and proficiency level, may be more helpful when there are no training instances available for the given word type.		
		
		%TODO unseen speakers?
		%Furthermore, in order for a CAPT tool to be useful in practice, it must be able to diagnose the speech of a learner from their very first time using the system. In the experiments reported above, it is possible that the training data’s inclusion of other utterances by the speaker in question resulted in higher performance than would be observed if such utterances were not available as training instances, so the classifier’s performance on unseen speakers must be compared to the performance reported above, once again with the hypothesis that the classifier will perform more poorly on unseen test data. To test these hypotheses, another series of experiments was carried out using different configurations of testing and training data sets.
		%To create training and testing data for experiments with utterances from unseen speakers, the entire set of labeled non-native utterances (see Section 4.4.1) was divided into 56 subsets, each containing all utterances from one of the 56 unique non-native speakers. Using a classifier trained on the native German utterances as well as those of the other 55 speakers, performance on each of these 56 test sets was evaluated. To investigate whether adding features related to speaker or word characteristics is more helpful than prosodic features alone when diagnosing unseen speakers’ errors, a comparison was made between classifiers trained using several combinations of the best features from the experiments described in Section 4.4.2 above. The results of the unseen-speaker experiments, and the different feature sets used for training, are given in Table 4.7.
		
		\subsection{Evaluation metrics}
		\label{sec:method:eval}
		
		Classifier performance is quantified in terms of percent accuracy (\% acc.) and Kappa agreement ($\kappa$) with respect to the gold-standard labels. For the [correct] class, 
	%precision (P), recall (R) and F$_1$ and F$_2$ measures are also reported
	the following measures are also reported:
	\begin{itemize}
			\item{Precision (P): number of utterances correctly classified as [correct] $/$ total no. classified as [correct]
			%the number of instances the classifier correctly assigned to this class (i.e. the number labeled as this class that were truly of this class), divided by the total number of instances it assigned to this class
			}
			\item{Recall (R): no. correctly classified as [correct] $/$ total no. of [correct] utterances in the gold-standard dataset
			%the number of instances the classifier correctly assigned to this class, divided by the number of instances which truly belong to this class
			}
			\item{F$_1$ measure:
			harmonic mean of P and R (where both are weighted equally), i.e. $F_1 = 2PR/(P+R)$
			%, also known simply as F-measure or F-score: a metric which combines Precision and Recall by taking their harmonic mean, weighting both equally. It is computed with the formula:
			%\[F_1 = 2PR/(P+R)\]
			%\[\frac{2PR}{P+R}\]
			}
			\item{F$_2$ measure: 
			similar to F$_1$ measure, but with R given twice as much weight as P: %
			% F$_2$ is computed as:
			\[F_2 = (1+2^2) \cdot PR/(2^2 \cdot P+R) = 5PR/(4P+R)\]
			}
	\end{itemize}
	These are reported to account for the fact that in the intended application of CAPT, telling a student that they have made a mistake when in fact they have not can be more damaging to their motivation and willingness to continue learning with the system than telling them that they have stressed a word correctly when in fact they have made a mistake \cite{Neri2002}. Therefore, [correct] R should be as close to 1 as possible, while still maintaining a balance with P such that the system does not trivially classify all utterances as [correct], which would render it useless. To keep this in perspective, the results in this section report both the commonly used F$_1$ measure, which weights P and R evenly, as well as F$_2$, which prioritizes R over P.
		

	\section{Results}
	\label{sec:results}
	
	This section describes the results of the classification experiments conducted by training CART classifiers using various configurations of the feature sets and train/test data splits described in \cref{sec:method}. 
	
		\subsection{Feature performance}
		\label{sec:results:features}		
		
		\begin{table}
			\centering
			\caption[Results of experiments with prosodic features]{Results of experiments with prosodic features.%,
			%quantified by percent accuracy (\% acc.) and Kappa agreement ($\kappa$) with respect to the gold-standard labels, as well as precision (P), recall (R) and F$_1$ and F$_2$ measures for the [correct] class. 
			The best values achieved for each metric are displayed in \textbf{bold}.
			}
			%\begin{tabularx}{\textwidth}{lXXXXXXXX}		
			\begin{tabularx}{\columnwidth}{lccXXXX}			
			
			\toprule
			\multirow{2}{*}{Feature set} & \multirow{2}{*}{\% acc.} & \multirow{2}{*}{$\kappa$} & \multicolumn{4}{c}{[correct] class} \\
			 %& acc. & inacc. 
			 \cmidrule(lr){4-7}
			& & & P & R & F$_1$ & F$_2$ \\
			\midrule
		
DR	&	66.78	&	0.19	&	0.69	&	0.91	&	0.79	&	0.86	\\
F0		&	64.37	&	0.02	&	0.64	&	\textbf{1.00}	&	0.78	&	\textbf{0.90}	\\
EN		&	63.77	&	0.00	&	0.64	&	\textbf{1.00}	&	0.78	&	\textbf{0.90}	\\
\addlinespace											
EN+F0		&	64.52	&	0.04	&	0.65	&	0.98	&	0.78	&	0.89	\\
DR+EN	&	67.68	&	0.25	&	0.71	&	0.89	&	0.79	&	0.85	\\
DR+F0		&	\textbf{69.77}	&	\textbf{0.29}	&	\textbf{0.72}	&	0.91	&	\textbf{0.80}	&	0.86	\\
\addlinespace											
%DR+F0+EN	
DR+F0+EN &	67.52	&	0.25	&	0.71	&	0.89	&	0.79	&	0.85	\\		
			\bottomrule
			\label{tab:results:prosody}
			\end{tabularx}
		\end{table}
		
		\Cref{tab:results:prosody} lists the results of experiments with the prosodic features described in \cref{tab:prosfeatures}. As seen in the first three rows of \cref{tab:results:prosody}, the results obtained using features representing each of the three acoustic correlates of lexical stress, duration, F0, and intensity (energy), confirm that duration features seem to be the best predictor of lexical stress errors.%, followed by F0 and then energy. 
		In fact, the perfect (1.00) R values and $\kappa$ at or near 0 for F0 and energy features seem to indicate that these feature sets do not enable the system to discriminate between error classes at all, resulting in classifiers that are useless for CAPT insofar as they simply classify all utterances as [correct].
		%(indeed, for these feature sets P for the [incorrect] class was 0.0). 
		%However, it should be noted that the F0 and intensity features do seem to be at an advantage over the duration features in one respect: the latter has an average recall of only 0.91 for [correct] utterances, while the other two features exhibit perfect recall (1.0). As mentioned earlier (Section 4.4.1), lower [correct] recall means that correct pronunciations are being misclassified as incorrect, which is a dangerous type of mistake for a CAPT system to make.
		
		However, as the lower rows of \cref{tab:results:prosody} show, better performance was obtained with classifiers trained on a combination of these features than using each set in isolation. The pairing of duration and F0 features, with energy features omitted, resulted in the best overall performance using only prosodic features: 
		69.77\% accuracy, $\kappa$=0.29, and [correct] F$_1$=0.8.
		%Compared to using each of the three prosodic feature sets in isolation, it is clear from the figures in the lower rows of Table 4.5 that even better performance can be achieved by combining these features. Interestingly, however, better performance was observed with classifiers trained only on duration and F0 features (omitting intensity features) than with those trained on features of all three types. This duration-F0 pairing resulted in the best- overall averages for any of the prosodic feature combinations: 69.77% accuracy, κ=0.29, and F1=0.8 for the [correct] class.
		
		
		\begin{table}%[tb]
			\centering
			\caption[Results of experiments with speaker and word features]{Results of experiments with speaker and word features. %,
			%quantified by percent accuracy (\% acc.) and Kappa agreement ($\kappa$) with respect to the gold-standard labels, as well as precision (P), recall (R) and F$_1$ and F$_2$ measures for the [correct] class. 
			Best values achieved for each metric are displayed in \textbf{bold}.
			}
			
					\begin{subtable}{\columnwidth}
			\centering
			\caption{In combination with DR+F0 feature set}
			\begin{tabularx}{\columnwidth}{lXXXXXX}		
			%\begin{tabular}{lcccccc}	
			\toprule
			Feature set & \multirow{2}{*}{\% acc.} & \multirow{2}{*}{$\kappa$} & \multicolumn{4}{c}{[correct] class} \\
			 %& acc. & inacc. 
			 \cmidrule(lr){4-7}
			(+DR+F0)& & & P & R & F$_1$ & F$_2$ \\
			\midrule
WD		&	70.52	&	0.30	&	\textbf{0.72}	&	0.92	&	\textbf{0.81}	&	\textbf{0.87}	\\
LV		&	68.72	&	0.27	&	0.71	&	0.91	&	0.79	&	0.86	\\
AG	&	68.26	&	0.22	&	0.69	&	\textbf{0.94}	&	0.80	&	0.88	\\
\addlinespace											
LV+AG	&	69.77	&	0.29	&	\textbf{0.72}	&	0.91	&	0.80	&	0.86	\\
WD+AG	&	68.86	&	0.27	&	0.71	&	0.91	&	0.80	&	0.86	\\
WD+LV	&	\textbf{70.65}	&	\textbf{0.31}	&	\textbf{0.72}	&	0.92	&	\textbf{0.81}	&	\textbf{0.87}	\\
\addlinespace											
WD+LV+AG	&	68.41	&	0.26	&	0.71	&	0.91	&	0.79	&	0.86	\\
			\bottomrule
			\label{tab:results:spkrword:durF0}
			\end{tabularx}
		\end{subtable}
			
			\begin{subtable}{\columnwidth}
			\centering
			\caption{In combination with DR+F0+EN feature set}
			\begin{tabularx}{\columnwidth}{lccXXXX}			
			%\begin{tabular}{lcccccc}
			\toprule
			Feature set & \multirow{2}{*}{\% acc.} & \multirow{2}{*}{$\kappa$} & \multicolumn{4}{c}{[correct] class} \\
			 %& acc. & inacc. 
			 \cmidrule(lr){4-7}
			(+DR+F0+EN)& & & P & R & F$_1$ & F$_2$ \\
			\midrule
WD	&	68.41	&	0.28	&	0.72	&	0.88	&	0.79	&	0.84	\\
LV	&	70.07	&	0.29	&	0.71	&	\textbf{0.92}	&	0.80	&	\textbf{0.87}	\\
AG	&	66.93	&	0.24	&	0.71	&	0.88	&	0.78	&	0.84	\\
\addlinespace													
LV+AG	&	68.57	&	0.27	&	0.72	&	0.89	&	0.79	&	0.85	\\
WD+AG	&	68.87	&	0.30	&	\textbf{0.73}	&	0.87	&	0.79	&	0.83	\\
WD+LV	&	\textbf{71.87}	&	\textbf{0.34}	&	\textbf{0.73}	&	\textbf{0.92}	&	\textbf{0.81}	&	\textbf{0.87}	\\		\addlinespace								
WD+LV+AG	&	70.52	&	0.31	&	0.72	&	0.91	&	0.80	&	0.86	\\
			\bottomrule
			\label{tab:results:spkrword:all}
			\end{tabularx}
		\end{subtable}
		
		\label{tab:results:spkrword}
	\end{table}
		
		As seen in \cref{tab:results:spkrword}, even higher accuracy was obtained by combining prosodic features with the features representing speaker and word characteristics (see \cref{tab:spkrfeatures}). Information about the word type of the utterance (WD) and the L2 German proficiency of the speaker (LV)%
		\footnote{\TODO{does this fit?} Though including the LV feature would be nonsensical if the intended application of the classifier were assessment of a learner's proficiency level, the goal here is not assessment but training, 
		%and learners of different levels may make different  have different training needs; therefore, 
		so in this case it is reasonable to allow the system to take LV into account. }		
		 seemed to be most helpful, while including the speaker's age/gender category (AG) appeared to have a negative, if any, impact on performance. 
		%Comparing the performance of the two speaker-related features, the speaker’s proficiency level (LV) seems to be a better predictor of stress accuracy than their age/gender category (GENDER, which refers to whether the speaker is a girl, boy, woman, or man, and not exclusively to whether they are a male or a female; see Table 4.4b). This is not surprising, considering the large discrepancy between skill level groups observed in the error distribution analysis (see Section 3.6.3). If the intended application of the classifier were the assessment of a learner’s proficiency level, including this feature would obviously be nonsensical; however, in this context the application is not assessment but training, and in that case it makes sense to allow the system to take the learner’s level into account.
		%Interestingly, for many of the word/speaker feature combinations tried, slightly different results were obtained when these features were combined with the full set of prosodic features (DR+F0+EN) than with the best-performing subset of the prosodic features, DR+F0. 
		Adding WD and LV to the best-performing prosodic features (DR+F0) improved classification accuracy slightly across all metrics; interestingly, however, the overall best performance on this dataset was achieved by combining WD and LV with the entire set of prosodic features (DR+F0+EN), yielding average accuracy of 71.87\%, $\kappa$ of 0.34, and [correct] F$_1$ and F$_2$ measures of 0.81 and 0.87, respectively.
		%For example, when combined with the feature set DR+F0+EN, the feature LV slightly outperformed the word of the utterance (WD) as a predictor of stress accuracy, yet when combined with only the duration and F0 features (DR+F0), WD outperformed LV. In both situations, the combination of these two features, WD+LV, seemed to give better results than any combination involving GENDER, including the combination using all three word/speaker features. In fact, the best-performing classifiers resulted from using the word of the utterance, the speaker’s proficiency level, and the entire set of prosodic features (WD+LV+DR+F0+EN), yielding an average accuracy of 71.87%, κ of 0.34, and F1 and F2 measures of 0.81 and 0.87, respectively.
		
		Though these statistics are the best of any of the experiments reported in this section, we would perhaps like to see better accuracy and F-measures, and higher than ``fair'' agreement \cite{Landis1977} with the gold-standard labels, before placing such an error-diagnosis system in front of actual students.
		% However, these results should be interpreted in the context of the particular task at hand, and as Neri et al. (2002, pp. 15-16) point out, for any automatic pronunciation scoring system to be useful, there must be a strong correlation between how the system evaluates a given utterance and how human raters would evaluate it. Here, classification performance was evaluated against the gold-standard annotation of the dataset; yet, as described in Section 3.5, the gold-standard labels represent a consolidation of multiple, often conflicting labels from different annotators (see Section 3.4). 		
		However, considering the relatively low agreement between humans tasked with the same type of error classification (see \cref{sec:data:agreement}), this accuracy does not seem so unimpressive.
		Indeed, the best average $\kappa$ between the classifier output and gold-standard labels (0.34) exceeds the observed average human-human $\kappa$ (0.23), and the best average percentage accuracy for that classifier (71.87\%) is substantially higher than the average human-human percentage agreement (54.92\%). 
		%TODO put the below in conclusion?
		%It seems possible, given the low inter-annotator agreement observed in the annotation described in Chapter 3, that there is an element of subjectivity in the decision of whether or not lexical stress was realized correctly in an utterance of a given word, such that an indisputable diagnosis of that utterance cannot be produced, and different listeners could and would make different assessments of that utterance. If this is the case, it would not be realistic to expect an automatic diagnosis system to reproduce humans’ assessments with perfect accuracy, and any agreement between the classifier’s output and the gold-standard labels which exceeds the average agreement between humans should be interpreted in a positive light.
		
		
		\subsection{Performance on unseen words}
		\label{sec:results:words}
		
		\begin{table}
			\centering
			\caption[Results of experiments with unseen words]{Results of experiments with unseen words. %, averaged over all 12 held-out word types.
			%Performance is quantified by percent accuracy (\% acc.) and Kappa agreement ($\kappa$) with respect to the gold-standard labels, as well as precision (P), recall (R) and F$_1$ and F$_2$ measures for the [correct] class. 
			The best values achieved for each metric are displayed in \textbf{bold}.
			}
			\begin{tabularx}{\columnwidth}{lccXXXX}		
			\toprule
			\multirow{2}{*}{Feature set} & \multirow{2}{*}{\% acc.} & \multirow{2}{*}{$\kappa$} & \multicolumn{4}{c}{[correct] class} \\
			\cmidrule(lr){4-7}
			& & & P & R & F$_1$ & F$_2$ \\
			\midrule
DR+F0	&	\textbf{66.85}	&	0.17	&	0.69	&	0.88	&	\textbf{0.77}	&	0.84	\\
+LV %{LV+DR+F0}	
	&	65.51	&	0.16	&	0.69	&	0.89	&	\textbf{0.77}	&	0.84	\\
+AG %{LVL+AG+DR+F0}	
	&	65.05	&	0.16	&	0.69	&	0.88	&	0.76	&	0.84	\\
			\midrule								
DR+F0+EN	&	65.66	&	\textbf{0.19}	&	\textbf{0.70}	&	0.85	&	0.76	&	0.82	\\
+LV %{LV+DR+F0+EN}	
	&	64.16	&	0.11	&	0.67	&	0.88	&	0.75	&	0.83	\\
+AG %{LVL+AG+DR+F0+EN}	
	&	64.31	&	0.12	&	0.68	&	\textbf{0.90}	&	\textbf{0.77}	&	\textbf{0.85}	\\

		\bottomrule
			\label{tab:results:words}
			\end{tabularx}
		\end{table}	
		
		\Cref{tab:results:words} presents the results of experiments with unseen words (see \cref{sec:method:datasets}), averaged over each of the 12 held-out word types.
		Interestingly, the highest accuracy (66.85\%) was achieved using classifiers trained only on the best-performing prosodic features (DR+F0), excluding the speaker-related features (LV and AG). However, classifiers trained on all three prosodic feature sets (DR+F0+EN) yielded the best agreement with the gold-standard labels ($\kappa$=0.19), while the best F-measures (F$_1$=0.77, F$_2$=0.85) were observed using classifiers trained using all available features (DR+F0+EN+LV+AG; WD was of course not included in these experiments).
		%As that table shows, the best average performance on all twelve unseen word types was achieved using classifiers trained only on the best-performing prosodic features (DR+F0), and not taking either of the speaker-related features into consideration: this yielded an accuracy of 66.85%, κ of 0.17, and F1 and F2 measures of 0.77 and 0.84, respectively.
		
		The drop in performance compared with the best results obtained on seen words (see \cref{tab:results:spkrword:all}) seems to indicate that extending classification to words for which L1 utterances are not available for training is not a trivial matter. 
		The difference in $\kappa$ is perhaps most striking, constituting a drop from ``fair'' to ``slight'' agreement with the gold standard \cite{Landis1977}, as well as a fall below the average inter-annotator $\kappa$ (0.23). However, the percentage accuracy still exceeds the average percentage agreement between human annotators (54.92\%), and the observed decreases in percentage accuracy and F-measures \TODO{with respect to seen words} do not seem drastic. This is encouraging for the prospect of classifying errors in novel word types; if overall classification performance can be improved by other means (e.g. more powerful machine learning algorithms, additional features), classifying errors in unseen words \TODO{may yet be feasible}.
		%In comparison with the best results obtained on seen words as reported in the previous section (69.77% accuracy and κ =0.29 for the DR+F0 feature set, and 71.87%/κ =0.34 for WD+LV+DR+F0+EN), these statistics seem to support the hypothesis that performance would be lower on words not seen in the training data. Perhaps most striking is the difference in κ, which constitutes a drop from fair to slight agreement with the gold-standard labels (using the Landis and Koch (1977) thresholds), which also constitutes a fall below the average inter-annotator κ of 0.23 observed between human annotations (see Section 3.4.1). However, the drop in the overall accuracy percentage, as well as in the two F-measures, seems less drastic, which seems encouraging from the perspective of extending the diagnostic capabilities of de-stress to novel word types using a classification-based approach, especially considering the possibilities for improving classification performance by other means, e.g. using additional features or more powerful machine learning algorithms (see Section 6.2).
		
		%\subsection{Performance on unknown speakers}
	


%		\subsection{Discussion}
%		\label{sec:results:discussion}
		
		

	\section{Conclusions and future work}
	\label{sec:conc}
	
		Classification of lexical stress errors using machine learning algorithms is a relatively novel approach to lexical stress error identification in German CAPT.
		%, in which a learner’s utterance is compared to the more abstract model of L1 speech represented by a classifier trained on a large number of L1 utterances. 
		This paper has 
		explored  
		%presented original contributions to the understanding of 
		how, and how effectively, classification-based diagnosis can be used to identify (in)correct realizations of lexical stress in the L2 German speech of L1 French speakers. 
		The prosodic features found to be most useful for classification relate to duration and F0, unsurprising considering that past work has indicated these may be the closest acoustic correlates of lexical stress in German \cite{Dogil1999,Cutler2005}. 
		Features representing the word type uttered and the L2 proficiency level of the speaker also seemed valuable for error classification; combining these features with the three prosodic feature types yielded the overall highest accuracy (71.87\% accuracy, $\kappa$=0.34) attained on the L2 speech dataset (see \cref{sec:data}).
		Though these results leave room for improvement, they are encouraging given that agreement between the classifier's output and the gold-standard labels slightly exceeded the average agreement observed between human annotators asked to perform the same error classification task (see \cref{sec:data:agreement}).
		%Features capturing the word type of the utterance as well as the age, gender and proficiency level of the speaker were also found to be quite valuable for error classification; combining these features with all three prosodic feature types resulted in the highest overall accuracy observed on this dataset (71.87\% accuracy, $\kappa$=0.34). As the observed agreement between the classifier’s labels and the gold standard slightly exceeded the overall inter-annotator agreement observed when humans were asked to perform this error diagnosis task (see Section 3.4), these results seem encouraging. 
		Somewhat lower classification accuracy was observed \TODO{with  utterances of word types} not represented in the training data, although this accuracy still remained comparable to the average percentage agreement between humans.
		The findings of the work reported here thus seem to \TODO{establish} the utility of classification-based diagnosis for lexical stress errors in German CAPT, though further work is needed to achieve the level of performance necessary for real-world CAPT systems.
		%Unsurprisingly, slightly lower accuracy was observed when classifying utterances of word types or speakers not represented in the training data; however, the fact that accuracy on unseen words still remained comparable to the human inter-annotator agreement statistics seems to confirm the expectation that classification-based diagnosis may be a useful way to create CAPT systems which are are not limited to words/sentences for which recorded L1 utterances are available.
	
	%\section{Future work}
	%\label{sec:future}

	One logical direction for future work is the evaluation of other, more powerful machine learning algorithms than the simple CARTs used in this work; related work indicates that Maximum Entropy classifiers \cite{Kim2011} and Neural Networks \cite{Shahin2012} may be promising. Consideration could also be given to additional features which may be related to lexical stress in German (e.g. spectral features capturing aspects of vowel quality). It may also be of interest to explore ensemble approaches to error classification, in which labels assigned by different classifiers trained only on subsets of the available features are compared to arrive at a final decision.
	%To obtain a more fine-grained analysis of the learner’s utterance in the classification approach, and thus enable the generation of additional types of feedback (e.g. skill bars) from classification-based diagnoses, different classifiers trained on only subsets of the avail- able features could be combined in an ensemble configuration; for example, an ensemble of three classifiers trained exclusively on duration, F0, and intensity features respectively could produce scores analogous to JSnoori’s three-dimensional pronunciation scores (Sec- tion 4.3.1). 
	%It may also be of interest to explore techniques for online, semi-supervised learning, which could enable the classifier to improve its predictions by updating itself with new training instances resulting from the learner’s continued use of the system over time.


  %TODO \section{Acknowledgements}
  



  %\newpage
  \eightpt
  \bibliographystyle{IEEEtran-nourl}

  \bibliography{../../library.bib}
  %\bibliography{slaterefs.bib}

%  \begin{thebibliography}{9}
%    \bibitem[1]{Davis80-COP}
%      S.\ B.\ Davis and P.\ Mermelstein,
%      ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%      \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
%    \bibitem[2]{Rabiner89-ATO}
%      L.\ R.\ Rabiner,
%      ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%      \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
%    \bibitem[3]{Hastie09-TEO}
%      T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%      \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%      New York: Springer, 2009.
%    \bibitem[4]{YourName15-XXX}
%      F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%      ``Title of your INTERSPEECH 2015 publication,''
%      in \textit{Interspeech 2015 -- 16\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 06--10, Dresden, Germany, Proceedings}, 2015, pp.~100--104.
%  \end{thebibliography}

\end{document}
